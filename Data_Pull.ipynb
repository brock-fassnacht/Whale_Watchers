{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import datetime\n",
    "\n",
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "import os\n",
    "import io\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling all SEC 13 F links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most Famous 13F investors\n",
    "investor_dict = {\n",
    "    \"1536411\": \"Stanley_Druckenmiller\",\n",
    "    \"1336528\": \"Bill_Ackman\",\n",
    "    \"1067983\": \"Warren_Buffet\",\n",
    "    \"1649339\": \"Michael_Burray\",\n",
    "    #\"921669\": \"Carl_Icahn\",\n",
    "    \"1656456\": \"David_Tepper\",\n",
    "    \"1040273\": \"Daniel_Loeb\",\n",
    "    \"1345471\": \"Nelson_Peltz\",\n",
    "    \"1541617\": \"Altimeter\", \n",
    "    \"1446114\": \"Ancora\",\n",
    "    \"1791786\": \"Elliot IM\",\n",
    "    \"1061165\": \"LONE PINE CAPITAL LLC\",\n",
    "    \"1135730\": \"COATUE MANAGEMENT LLC\",\n",
    "    #\"1015370\": \"GREENLIGHT CAPITAL, L.P.\",\n",
    "    \"1387322\": \"Whale Rock Capital Management LLC\",\n",
    "    \"1802278\": \"Stokes Family Office, LLC\",\n",
    "    \"1103804\": \"VIKING GLOBAL INVESTORS LP\",\n",
    "    \"1540866\": \"Makaira Partners LLC\",\n",
    "    \"1697868\": \"Valley Forge Capital Management, LP\",\n",
    "    \"1709323\": \"Himalaya Capital Management LLC\",\n",
    "    \"1063296\": \"ATLANTIC INVESTMENT MANAGEMENT, INC.\",\n",
    "    \"1115373\": \"SEMPER AUGUSTUS INVESTMENTS GROUP LLC\",\n",
    "    \"1631664\": \"Punch Card Management L.P.\",\n",
    "    \"1056831\": \"FAIRHOLME CAPITAL MANAGEMENT LLC\",\n",
    "    \"1553733\": \"Brave Warrior Advisors, LLC\", \n",
    "    \"1559771\": \"Engaged Capital LLC\",\n",
    "    \"1631014\": \"ALTAROCK PARTNERS LP\",\n",
    "    \"1039565\" : \"KAHN BROTHERS GROUP INC\",\n",
    "    \"1641864\" : \"Giverny Capital Inc.\",\n",
    "    \"1167483\": \"TIGER GLOBAL MANAGEMENT LLC\",\n",
    "    \"947996\": \"Olstein Capital Management, L.P.\",\n",
    "    \"846222\": \"GREENHAVEN ASSOCIATES INC\", #### Looks interesting\n",
    "    \"1484150\": \"Lindsell Train Ltd\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling reported porfolio values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cik_data_pull2(acik, filing_name):\n",
    "    # Constants\n",
    "    CIK = acik\n",
    "    SEC_API_URL = f\"https://data.sec.gov/submissions/CIK000{CIK}.json\"\n",
    "\n",
    "    # Headers for the SEC API request\n",
    "    headers = {'User-Agent': \"bfassnacht17@gmail.com\"}\n",
    "\n",
    "    base_url = \"https://sec.gov\"\n",
    "\n",
    "    # Function to get 13F filings\n",
    "    def get_13f_filings(cik):\n",
    "        url = f\"https://data.sec.gov/submissions/CIK000{cik}.json\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract recent filings data\n",
    "            forms = data['filings']['recent']['form']\n",
    "            accession_numbers = data['filings']['recent']['accessionNumber']\n",
    "            filing_dates = data['filings']['recent']['filingDate']\n",
    "            \n",
    "            # Create a list to store filing data\n",
    "            filings_data = []\n",
    "            \n",
    "            # Loop through filings and filter for 13F-HR forms\n",
    "            for i, form in enumerate(forms):\n",
    "                if form == \"13F-HR\":\n",
    "                    accession_number = accession_numbers[i]\n",
    "                    filing_date = filing_dates[i]\n",
    "                    \n",
    "                    # Construct the link to the filing's index page\n",
    "                    filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_number}/{accession_number}-index.htm\"\n",
    "                    \n",
    "                    # Parse the filing index page to find the 13F XML file link\n",
    "                    form_13f_url = get_form_13f_url(filing_url)\n",
    "                    \n",
    "                    # Append the data to the list\n",
    "                    filings_data.append({\n",
    "                        'Form': form,\n",
    "                        'Filing Date': filing_date,\n",
    "                        'Filing Index URL': filing_url,\n",
    "                        'Form 13F URL': form_13f_url\n",
    "                    })\n",
    "            \n",
    "            # Convert the list to a DataFrame\n",
    "            df_filings = pd.DataFrame(filings_data)\n",
    "            return df_filings\n",
    "        else:\n",
    "            print(\"Failed to retrieve data.\")\n",
    "            return None\n",
    "\n",
    "    # Function to extract the Form 13F XML file URL from the index page\n",
    "    def get_form_13f_url(index_url):\n",
    "        response = requests.get(index_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for the link to the Form 13F XML file\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href and ('slform13f' in href.lower() and href.endswith('.xml') and 'primary_doc' not in href.lower()):\n",
    "                    # Construct the full URL\n",
    "                \n",
    "                    full_url = f\"{base_url}/{href}\"\n",
    "                    return full_url\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Get 13F filings\n",
    "    df_13f = get_13f_filings(CIK)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if df_13f is not None:\n",
    "        df_13f_filtered = df_13f[df_13f[\"Form 13F URL\"].notna()]\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for index, row in df_13f_filtered.iterrows():\n",
    "\n",
    "            url = row[\"Form 13F URL\"]\n",
    "            filing_date = row[\"Filing Date\"]\n",
    "            \n",
    "            # Fetching the XML content from the URL\n",
    "            response = requests.get(url, headers=headers)\n",
    "            xml_content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object\n",
    "            soup = BeautifulSoup(xml_content, 'html.parser')\n",
    "\n",
    "            # Find the table element containing the data\n",
    "            table = soup.find('table', summary=\"Form 13F-NT Header Information\")\n",
    "\n",
    "            # Extract the column headers\n",
    "            header_row = table.find('tr')\n",
    "            header_cells = header_row.find_all('td')\n",
    "            column_headers = [cell.text.strip() for cell in header_cells]\n",
    "\n",
    "            # Extract the data rows\n",
    "            data_rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "            data = []\n",
    "            for row in data_rows:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                data.append(row_data)   \n",
    "\n",
    "            # Create a pandas DataFrame\n",
    "            df = pd.DataFrame(data[2:], columns=data[1])\n",
    "\n",
    "            # Handle non-breaking space characters (if present)\n",
    "            df = df.replace('\\xa0', '', regex=True)\n",
    "\n",
    "            df = df.rename(columns={\n",
    "                '(to the nearest dollar)': 'VALUE', \n",
    "                '(x$1000)': 'VALUE'\n",
    "            })\n",
    "            \n",
    "            dfs.append([filing_date, df])\n",
    "\n",
    "\n",
    "\n",
    "        ## Pulling data frame of historical holdings\n",
    "        filing_dates = [date[0] for date in dfs]\n",
    "        columns = filing_dates\n",
    "\n",
    "        all_holdings = pd.concat([df[1].loc[:, [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"]] for df in dfs]).drop_duplicates()\n",
    "        all_holdings_values = all_holdings.copy()\n",
    "\n",
    "        for i in range(len(dfs)):\n",
    "            new = dfs[i][1][[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", \"PRN AMT\"]].copy()\n",
    "\n",
    "            new[\"PRN AMT\"] = new[\"PRN AMT\"].str.replace(',', '')  # Replace commas in entire column\n",
    "            \n",
    "            new.columns = [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", columns[i]]\n",
    "            new[columns[i]] = new[columns[i]].astype(np.int64)  # Convert entire column to integer\n",
    "\n",
    "            new1 = new.groupby([\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"])[columns[i]].sum().reset_index()\n",
    "            all_holdings = all_holdings.merge(new1, on=[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"], how='left')\n",
    "\n",
    "        \n",
    "\n",
    "        all_holdings.to_csv(f\"C:\\\\Users\\\\bfass\\\\OneDrive\\\\Desktop\\\\Fin tools\\\\BigMoney13F\\\\Holdings_shares\\\\{filing_name}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(len(dfs)):\n",
    "            new = dfs[i][1][[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", \"VALUE\"]].copy()\n",
    "\n",
    "\n",
    "            new[\"VALUE\"] = new[\"VALUE\"].str.replace(',', '')  # Replace commas in entire column\n",
    "            new.columns = [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", columns[i]]\n",
    "\n",
    "\n",
    "            new[columns[i]] = new[columns[i]].astype(np.int64)  # Convert entire column to integer\n",
    "\n",
    "            new1 = new.groupby([\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"])[columns[i]].sum().reset_index()\n",
    "            all_holdings_values = all_holdings_values.merge(new1, on=[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"], how='left')\n",
    "\n",
    "       \n",
    "\n",
    "        all_holdings_values.to_csv(f\"C:\\\\Users\\\\bfass\\\\OneDrive\\\\Desktop\\\\Fin tools\\\\BigMoney13F\\\\Holdings_value\\\\{filing_name}.csv\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "for cik, whale in investor_dict.items():\n",
    "    #print(whale)\n",
    "    cik_data_pull2(cik, whale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Downloading Zip files from web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "downloadUrl = 'https://www.sec.gov/data-research/sec-markets-data/fails-deliver-data'\n",
    "\n",
    "headers = {'User-Agent': \"bfassnacht17@gmail.com\"}\n",
    "\n",
    "req = requests.get(downloadUrl, headers=headers)\n",
    "filename = req.url[downloadUrl.rfind('/')+1:]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 32 ZIP files.\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202408b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202408b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202408a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202408a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202308b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202308b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202308a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202308a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202208b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202208b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202208a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202208a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202108b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202108b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202108a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202108a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202008b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202008b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202008a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails202008a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201908b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201908b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201908a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201908a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201808b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201808b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201808a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201808a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201708b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201708b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201708a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201708a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201608b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201608b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201608a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201608a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201508b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201508b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201508a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201508a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201408b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201408b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201408a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201408a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201308b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201308b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201308a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201308a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201208b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201208b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201208a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201208a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201108b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201108b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201108a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201108a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201008b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201008b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201008a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails201008a.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails200908b.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails200908b.zip\n",
      "Downloading C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails200908a.zip ...\n",
      "Saved C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker\\cnsfails200908a.zip\n",
      "All files downloaded.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "\n",
    "# Get the current date and time\n",
    "now = datetime.now()\n",
    "\n",
    "# Get the previous month by subtracting one month\n",
    "previous_month_date = now - relativedelta(months=1)\n",
    "previous_month_number = previous_month_date.strftime(\"%m\")\n",
    "\n",
    "\n",
    "\n",
    "# Set the SEC page URL\n",
    "download_url = 'https://www.sec.gov/data-research/sec-markets-data/fails-deliver-data'\n",
    "\n",
    "headers = {'User-Agent': \"bfassnacht17@gmail.com\"}\n",
    "\n",
    "# Make a GET request to fetch the page content\n",
    "req = requests.get(download_url, headers=headers)\n",
    "\n",
    "# Parse the HTML content using BeautifulSoup\n",
    "soup = BeautifulSoup(req.content, 'html.parser')\n",
    "\n",
    "# Find all the links to ZIP files (assuming they end with .zip)\n",
    "zip_links = []\n",
    "for link in soup.find_all('a', href=True):\n",
    "    if link['href'].endswith('.zip') and ((previous_month_number + \"a\" in link['href']) or (previous_month_number + \"b\" in link['href'])):\n",
    "        zip_links.append(link['href'])\n",
    "\n",
    "# Check if any ZIP links were found\n",
    "if not zip_links:\n",
    "    print(\"No ZIP file links found on the page.\")\n",
    "else:\n",
    "    print(f\"Found {len(zip_links)} ZIP files.\")\n",
    "\n",
    "# Create a directory to save the ZIP files if it doesn't exist\n",
    "output_dir = r'C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker'\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Download each ZIP file\n",
    "for zip_link in zip_links:\n",
    "    # If the link is relative, convert it to an absolute URL\n",
    "    zip_url = zip_link if zip_link.startswith('http') else f'https://www.sec.gov{zip_link}'\n",
    "    \n",
    "    # Get the ZIP file name\n",
    "    zip_file_name = os.path.join(output_dir, zip_url.split('/')[-1])\n",
    "    \n",
    "    # Download the ZIP file\n",
    "    print(f\"Downloading {zip_file_name} ...\")\n",
    "    zip_req = requests.get(zip_url, headers=headers)\n",
    "    \n",
    "    # Save the ZIP file to the output directory\n",
    "    with open(zip_file_name, 'wb') as f:\n",
    "        f.write(zip_req.content)\n",
    "    \n",
    "    print(f\"Saved {zip_file_name}\")\n",
    "\n",
    "print(\"All files downloaded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataFrames successfully unioned!\n"
     ]
    }
   ],
   "source": [
    "# Directory where the text files are stored\n",
    "directory_path = r'C:\\Users\\bfass\\OneDrive\\Desktop\\Fin tools\\BigMoney13F\\Cusip_to_ticker'\n",
    "dataframes = []\n",
    "\n",
    "# Loop through each ZIP file in the directory\n",
    "for filename in os.listdir(directory_path):\n",
    "    if filename.endswith('.zip'):  # Ensure we only process ZIP files\n",
    "        zip_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Open the ZIP file\n",
    "        with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
    "            # Get the list of files in the ZIP (assuming one text file per ZIP)\n",
    "            text_files = zip_ref.namelist()\n",
    "            \n",
    "            # Make sure there is exactly one text file in the ZIP\n",
    "            if len(text_files) == 1:\n",
    "                # Extract the text file directly into memory\n",
    "                with zip_ref.open(text_files[0]) as f:\n",
    "                    # Try using 'ISO-8859-1' encoding to handle non-UTF-8 characters\n",
    "                    with io.TextIOWrapper(f, encoding=\"ISO-8859-1\") as text_file:\n",
    "                        df = pd.read_csv(text_file, delimiter='|')\n",
    "                    \n",
    "                        # Append the DataFrame to the list\n",
    "                        dataframes.append(df)\n",
    "\n",
    "# Concatenate all DataFrames in the list into a single DataFrame\n",
    "unioned_df = pd.concat(dataframes, ignore_index=True)\n",
    "\n",
    "# Optionally, save the final unioned DataFrame to a CSV file\n",
    "# unioned_df.to_csv('unioned_output.csv', index=False)\n",
    "\n",
    "print(\"DataFrames successfully unioned!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bfass\\AppData\\Local\\Temp\\ipykernel_14104\\2862455774.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  unioned_df2[\"SETTLEMENT DATE\"] = pd.to_datetime(unioned_df2[\"SETTLEMENT DATE\"])\n"
     ]
    }
   ],
   "source": [
    "unioned_df2 = unioned_df[unioned_df[\"SETTLEMENT DATE\"].str.len() == 8]\n",
    "\n",
    "unioned_df2[\"SETTLEMENT DATE\"] = pd.to_datetime(unioned_df2[\"SETTLEMENT DATE\"])\n",
    "\n",
    "recent_cusip = unioned_df2.groupby([\"SYMBOL\", \"CUSIP\"])[\"SETTLEMENT DATE\"].max().reset_index()\n",
    "recent_cusip.columns = [\"SYMBOL\", \"CUSIP\", \"Recent_Date\"]\n",
    "\n",
    "unioned_df2 = unioned_df2.merge(recent_cusip, on=[\"SYMBOL\", \"CUSIP\"], how='inner')\n",
    "\n",
    "unioned_df2.to_csv(r\"Final_cusip_map\\CUSIP_Mapping.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
