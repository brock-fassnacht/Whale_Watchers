{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import xml.etree.ElementTree as ET\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling all SEC 13 F links"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Most Famous 13F investors\n",
    "investor_dict = {\n",
    "    \"1536411\": \"Stanley_Druckenmiller\",\n",
    "    \"1336528\": \"Bill_Ackman\",\n",
    "    \"1067983\": \"Warren_Buffet\",\n",
    "    \"1649339\": \"Michael_Burray\",\n",
    "    #\"921669\": \"Carl_Icahn\",\n",
    "    \"1656456\": \"David_Tepper\",\n",
    "    \"1040273\": \"Daniel_Loeb\",\n",
    "    \"1345471\": \"Nelson_Peltz\"\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pulling reported porfolio values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cik_data_pull2(acik, filing_name):\n",
    "    # Constants\n",
    "    CIK = acik\n",
    "    SEC_API_URL = f\"https://data.sec.gov/submissions/CIK000{CIK}.json\"\n",
    "\n",
    "    # Headers for the SEC API request\n",
    "    headers = {'User-Agent': \"bfassnacht17@gmail.com\"}\n",
    "\n",
    "    base_url = \"https://sec.gov\"\n",
    "\n",
    "    # Function to get 13F filings\n",
    "    def get_13f_filings(cik):\n",
    "        url = f\"https://data.sec.gov/submissions/CIK000{cik}.json\"\n",
    "        response = requests.get(url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            data = response.json()\n",
    "            \n",
    "            # Extract recent filings data\n",
    "            forms = data['filings']['recent']['form']\n",
    "            accession_numbers = data['filings']['recent']['accessionNumber']\n",
    "            filing_dates = data['filings']['recent']['filingDate']\n",
    "            \n",
    "            # Create a list to store filing data\n",
    "            filings_data = []\n",
    "            \n",
    "            # Loop through filings and filter for 13F-HR forms\n",
    "            for i, form in enumerate(forms):\n",
    "                if form == \"13F-HR\":\n",
    "                    accession_number = accession_numbers[i]\n",
    "                    filing_date = filing_dates[i]\n",
    "                    \n",
    "                    # Construct the link to the filing's index page\n",
    "                    filing_url = f\"https://www.sec.gov/Archives/edgar/data/{int(cik)}/{accession_number}/{accession_number}-index.htm\"\n",
    "                    \n",
    "                    # Parse the filing index page to find the 13F XML file link\n",
    "                    form_13f_url = get_form_13f_url(filing_url)\n",
    "                    \n",
    "                    # Append the data to the list\n",
    "                    filings_data.append({\n",
    "                        'Form': form,\n",
    "                        'Filing Date': filing_date,\n",
    "                        'Filing Index URL': filing_url,\n",
    "                        'Form 13F URL': form_13f_url\n",
    "                    })\n",
    "            \n",
    "            # Convert the list to a DataFrame\n",
    "            df_filings = pd.DataFrame(filings_data)\n",
    "            return df_filings\n",
    "        else:\n",
    "            print(\"Failed to retrieve data.\")\n",
    "            return None\n",
    "\n",
    "    # Function to extract the Form 13F XML file URL from the index page\n",
    "    def get_form_13f_url(index_url):\n",
    "        response = requests.get(index_url, headers=headers)\n",
    "        \n",
    "        if response.status_code == 200:\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Look for the link to the Form 13F XML file\n",
    "            links = soup.find_all('a')\n",
    "            \n",
    "            for link in links:\n",
    "                href = link.get('href')\n",
    "                if href and ('slform13f' in href.lower() and href.endswith('.xml') and 'primary_doc' not in href.lower()):\n",
    "                    # Construct the full URL\n",
    "                \n",
    "                    full_url = f\"{base_url}/{href}\"\n",
    "                    return full_url\n",
    "        \n",
    "        return None\n",
    "\n",
    "    # Get 13F filings\n",
    "    df_13f = get_13f_filings(CIK)\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    if df_13f is not None:\n",
    "        df_13f_filtered = df_13f[df_13f[\"Form 13F URL\"].notna()]\n",
    "\n",
    "        dfs = []\n",
    "\n",
    "        for index, row in df_13f_filtered.iterrows():\n",
    "\n",
    "            url = row[\"Form 13F URL\"]\n",
    "            filing_date = row[\"Filing Date\"]\n",
    "            \n",
    "            # Fetching the XML content from the URL\n",
    "            response = requests.get(url, headers=headers)\n",
    "            xml_content = response.content\n",
    "\n",
    "            # Create a BeautifulSoup object\n",
    "            soup = BeautifulSoup(xml_content, 'html.parser')\n",
    "\n",
    "            # Find the table element containing the data\n",
    "            table = soup.find('table', summary=\"Form 13F-NT Header Information\")\n",
    "\n",
    "            # Extract the column headers\n",
    "            header_row = table.find('tr')\n",
    "            header_cells = header_row.find_all('td')\n",
    "            column_headers = [cell.text.strip() for cell in header_cells]\n",
    "\n",
    "            # Extract the data rows\n",
    "            data_rows = table.find_all('tr')[1:]  # Skip the header row\n",
    "            data = []\n",
    "            for row in data_rows:\n",
    "                cells = row.find_all('td')\n",
    "                row_data = [cell.text.strip() for cell in cells]\n",
    "                data.append(row_data)   \n",
    "\n",
    "            # Create a pandas DataFrame\n",
    "            df = pd.DataFrame(data[2:], columns=data[1])\n",
    "\n",
    "            # Handle non-breaking space characters (if present)\n",
    "            df = df.replace('\\xa0', '', regex=True)\n",
    "\n",
    "            df = df.rename(columns={\n",
    "                '(to the nearest dollar)': 'VALUE', \n",
    "                '(x$1000)': 'VALUE'\n",
    "            })\n",
    "            \n",
    "            dfs.append([filing_date, df])\n",
    "\n",
    "\n",
    "\n",
    "        ## Pulling data frame of historical holdings\n",
    "        filing_dates = [date[0] for date in dfs]\n",
    "        columns = filing_dates\n",
    "\n",
    "        all_holdings = pd.concat([df[1].loc[:, [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"]] for df in dfs]).drop_duplicates()\n",
    "        all_holdings_values = all_holdings.copy()\n",
    "\n",
    "        for i in range(len(dfs)):\n",
    "            new = dfs[i][1][[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", \"PRN AMT\"]].copy()\n",
    "\n",
    "            new[\"PRN AMT\"] = new[\"PRN AMT\"].str.replace(',', '')  # Replace commas in entire column\n",
    "            \n",
    "            new.columns = [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", columns[i]]\n",
    "            new[columns[i]] = new[columns[i]].astype(np.int64)  # Convert entire column to integer\n",
    "\n",
    "            new1 = new.groupby([\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"])[columns[i]].sum().reset_index()\n",
    "            all_holdings = all_holdings.merge(new1, on=[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"], how='left')\n",
    "\n",
    "        all_holdings = all_holdings.fillna(0).sort_values(by=\"2024-08-14\", ascending=False)\n",
    "\n",
    "        all_holdings.to_csv(f\"C:\\\\Users\\\\bfass\\\\OneDrive\\\\Desktop\\\\Fin tools\\\\BigMoney13F\\\\Holdings_shares\\\\{filing_name}.csv\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "\n",
    "        for i in range(len(dfs)):\n",
    "            new = dfs[i][1][[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", \"VALUE\"]].copy()\n",
    "\n",
    "\n",
    "            new[\"VALUE\"] = new[\"VALUE\"].str.replace(',', '')  # Replace commas in entire column\n",
    "            new.columns = [\"NAME OF ISSUER\", \"CUSIP\", \"CALL\", columns[i]]\n",
    "\n",
    "\n",
    "            new[columns[i]] = new[columns[i]].astype(np.int64)  # Convert entire column to integer\n",
    "\n",
    "            new1 = new.groupby([\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"])[columns[i]].sum().reset_index()\n",
    "            all_holdings_values = all_holdings_values.merge(new1, on=[\"NAME OF ISSUER\", \"CUSIP\", \"CALL\"], how='left')\n",
    "\n",
    "        all_holdings_values = all_holdings_values.fillna(0).sort_values(by=\"2024-08-14\", ascending=False)\n",
    "\n",
    "        all_holdings_values.to_csv(f\"C:\\\\Users\\\\bfass\\\\OneDrive\\\\Desktop\\\\Fin tools\\\\BigMoney13F\\\\Holdings_value\\\\{filing_name}.csv\")\n",
    "\n",
    "\n",
    "    else:\n",
    "        return None\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stanley_Druckenmiller\n",
      "Bill_Ackman\n",
      "Warren_Buffet\n",
      "Michael_Burray\n",
      "David_Tepper\n",
      "Daniel_Loeb\n",
      "Nelson_Peltz\n"
     ]
    }
   ],
   "source": [
    "for cik, whale in investor_dict.items():\n",
    "    #print(whale)\n",
    "    cik_data_pull2(cik, whale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
